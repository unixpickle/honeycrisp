#include <metal_stdlib>
using namespace metal;

inline float safe_tanh(float x) {
    return (x < -10 ? -1 : (x > 10 ? 1 : tanh(x)));
}

template <typename T>
inline T pythonFmod(T lhs, T rhs) {
    if (rhs < 0) {
        return -pythonFmod(-lhs, -rhs);
    } else if (lhs < 0) {
        return fmod(rhs - fmod(rhs - lhs, rhs), rhs);
    } else {
        return fmod(lhs, rhs);
    }
}

template <typename T>
inline T pythonIntMod(T lhs, T rhs) {
    if (rhs < 0) {
      return -pythonIntMod(-lhs, -rhs);
    } else if (lhs < 0) {
      return (rhs - ((rhs - lhs) % rhs)) % rhs;
    } else {
      return lhs % rhs;
    }
}

struct bcast_strides {
    uint numDims;
    uint shape[8];
    uint strides[8];
};

inline uint bcast_strides_index(struct bcast_strides strides, uint rawIndex) {
    if (strides.numDims == 1) {
        return rawIndex * strides.strides[0];
    }
    uint result = 0;
    uint curIndex = rawIndex;
    for (int i = (int)strides.numDims - 1; i >= 0; i--) {
        result += strides.strides[i] * (curIndex % strides.shape[i]);
        curIndex /= strides.shape[i];
    }
    return result;
}

#define BROADCAST_KERNEL(type) \
    kernel void broadcast_##type( \
        device const type* a [[buffer(0)]], \
        device type* b [[buffer(1)]], \
        constant struct bcast_strides &strides [[buffer(2)]], \
        constant uint &N [[buffer(3)]], \
        uint id [[thread_position_in_grid]] \
    ) { \
        if (id < N) { \
            b[id] = a[bcast_strides_index(strides, id)]; \
        } \
    } \

BROADCAST_KERNEL(char)
BROADCAST_KERNEL(short)
BROADCAST_KERNEL(int)
BROADCAST_KERNEL(long)

#define BINARY_KERNELS(name, expr, type, outType) \
    struct name##vv_args_##type { \
        struct bcast_strides aStrides; \
        struct bcast_strides bStrides; \
        uint N; \
    }; \
    kernel void name##_vv_##type( \
        device const type* a [[buffer(0)]], \
        device const type* b [[buffer(1)]], \
        device outType* c [[buffer(2)]], \
        constant struct name##vv_args_##type &args [[buffer(3)]], \
        uint id [[thread_position_in_grid]] \
    ) { \
        if (id < args.N) { \
            type x = a[bcast_strides_index(args.aStrides, id)]; \
            type y = b[bcast_strides_index(args.bStrides, id)]; \
            c[id] = expr; \
        } \
    } \
    kernel void name##_vs_##type( \
        device const type* a [[buffer(0)]], \
        device const float& b [[buffer(1)]], \
        device outType* c [[buffer(2)]], \
        constant uint &N [[buffer(3)]], \
        uint id [[thread_position_in_grid]] \
    ) { \
        if (id < N) { \
            type x = a[id]; \
            type y = (type)b; \
            c[id] = expr; \
        } \
    } \
    kernel void name##_sv_##type( \
        device const float& a [[buffer(0)]], \
        device const type* b [[buffer(1)]], \
        device outType* c [[buffer(2)]], \
        constant uint &N [[buffer(3)]], \
        uint id [[thread_position_in_grid]] \
    ) { \
        if (id < N) { \
            type x = (type)a; \
            type y = b[id]; \
            c[id] = expr; \
        } \
    }

#define ALL_BINARY_KERNELS_FOR_IN_TYPE(type) \
    BINARY_KERNELS(add, x+y, type, type) \
    BINARY_KERNELS(sub, x-y, type, type) \
    BINARY_KERNELS(mul, x*y, type, type) \
    BINARY_KERNELS(div, x/y, type, type) \
    BINARY_KERNELS(lt, x<y, type, char) \
    BINARY_KERNELS(gt, x>y, type, char) \
    BINARY_KERNELS(le, x<=y, type, char) \
    BINARY_KERNELS(ge, x>=y, type, char) \
    BINARY_KERNELS(eq, x==y, type, char)

ALL_BINARY_KERNELS_FOR_IN_TYPE(half)
ALL_BINARY_KERNELS_FOR_IN_TYPE(float)
ALL_BINARY_KERNELS_FOR_IN_TYPE(long)
BINARY_KERNELS(mod, (pythonFmod(x,y)), half, half)
BINARY_KERNELS(mod, (pythonFmod(x,y)), float, float)
BINARY_KERNELS(mod, (pythonIntMod(x,y)), long, long)

#define BITWISE_KERNELS(name, expr, type) \
    struct name##_args_##type { \
        struct bcast_strides aStrides; \
        struct bcast_strides bStrides; \
        uint N; \
    }; \
    kernel void name##_##type( \
        device const type* a [[buffer(0)]], \
        device const type* b [[buffer(1)]], \
        device type* c [[buffer(2)]], \
        constant struct name##_args_##type &args [[buffer(3)]], \
        uint id [[thread_position_in_grid]] \
    ) { \
        if (id < args.N) { \
            type x = a[bcast_strides_index(args.aStrides, id)]; \
            type y = b[bcast_strides_index(args.bStrides, id)]; \
            c[id] = expr; \
        } \
    }

#define ALL_BITWISE_KERNELS(type) \
    BITWISE_KERNELS(xor, x^y, type) \
    BITWISE_KERNELS(or, x|y, type) \
    BITWISE_KERNELS(and, x&y, type)

ALL_BITWISE_KERNELS(char)
ALL_BITWISE_KERNELS(short)
ALL_BITWISE_KERNELS(int)
ALL_BITWISE_KERNELS(long)

#define DEFINE_FUSED_ADD_MUL(type) \
    kernel void add_mul_##type( \
        device const type* a [[buffer(0)]], \
        device const type* b [[buffer(1)]], \
        device const type* c [[buffer(2)]], \
        device type* output [[buffer(3)]], \
        constant struct bcast_strides &aStrides [[buffer(4)]], \
        constant struct bcast_strides &bStrides [[buffer(5)]], \
        constant struct bcast_strides &cStrides [[buffer(6)]], \
        constant uint &N [[buffer(7)]], \
        uint id [[thread_position_in_grid]] \
    ) { \
        if (id < N) { \
            output[id] = (a[bcast_strides_index(aStrides, id)] + b[bcast_strides_index(bStrides, id)]) * c[bcast_strides_index(cStrides, id)]; \
        } \
    } \
    kernel void mul_add_##type( \
        device const type* a [[buffer(0)]], \
        device const type* b [[buffer(1)]], \
        device const type* c [[buffer(2)]], \
        device type* output [[buffer(3)]], \
        constant struct bcast_strides &aStrides [[buffer(4)]], \
        constant struct bcast_strides &bStrides [[buffer(5)]], \
        constant struct bcast_strides &cStrides [[buffer(6)]], \
        constant uint &N [[buffer(7)]], \
        uint id [[thread_position_in_grid]] \
    ) { \
        if (id < N) { \
            output[id] = (a[bcast_strides_index(aStrides, id)] * b[bcast_strides_index(bStrides, id)]) + c[bcast_strides_index(cStrides, id)]; \
        } \
    }

DEFINE_FUSED_ADD_MUL(long)
DEFINE_FUSED_ADD_MUL(float)
DEFINE_FUSED_ADD_MUL(half)

#define DEFINE_NORMALIZE(type) \
    struct normalize_args_##type { \
        float epsilon; \
        struct bcast_strides inputStrides; \
        struct bcast_strides meanStrides; \
        struct bcast_strides varianceStrides; \
        uint N; \
    }; \
    struct normalize_x_grad_args_##type { \
        float epsilon; \
        float sign; \
        struct bcast_strides varianceStrides; \
        struct bcast_strides outGradStrides; \
        uint N; \
    }; \
    struct normalize_var_grad_args_##type { \
        float epsilon; \
        struct bcast_strides inputStrides; \
        struct bcast_strides meanStrides; \
        struct bcast_strides varianceStrides; \
        struct bcast_strides outGradStrides; \
        uint N; \
    }; \
    kernel void normalize_##type( \
        device const type* input [[buffer(0)]], \
        device const type* mean [[buffer(1)]], \
        device const type* variance [[buffer(2)]], \
        device type* output [[buffer(3)]], \
        constant struct normalize_args_##type &args [[buffer(4)]], \
        uint id [[thread_position_in_grid]] \
    ) { \
        if (id < args.N) { \
            type x = (float)input[bcast_strides_index(args.inputStrides, id)]; \
            type mu = (float)mean[bcast_strides_index(args.meanStrides, id)]; \
            type sig = (float)variance[bcast_strides_index(args.varianceStrides, id)]; \
            output[id] = (type)((x - mu) / sqrt(sig + args.epsilon)); \
        } \
    } \
    kernel void normalize_x_grad_##type( \
        device const type* variance [[buffer(0)]], \
        device const type* outGrad [[buffer(1)]], \
        device type* output [[buffer(2)]], \
        constant struct normalize_x_grad_args_##type &args [[buffer(3)]], \
        uint id [[thread_position_in_grid]] \
    ) { \
        if (id < args.N) { \
            type sig = (float)variance[bcast_strides_index(args.varianceStrides, id)]; \
            type g = (float)outGrad[bcast_strides_index(args.outGradStrides, id)]; \
            output[id] = (type)(args.sign * g / sqrt(sig + args.epsilon)); \
        } \
    } \
    kernel void normalize_var_grad_##type( \
        device const type* input [[buffer(0)]], \
        device const type* mean [[buffer(1)]], \
        device const type* variance [[buffer(2)]], \
        device const type* outGrad [[buffer(3)]], \
        device type* output [[buffer(4)]], \
        constant struct normalize_var_grad_args_##type &args [[buffer(5)]], \
        uint id [[thread_position_in_grid]] \
    ) { \
        if (id < args.N) { \
            type x = (float)input[bcast_strides_index(args.inputStrides, id)]; \
            type mu = (float)mean[bcast_strides_index(args.meanStrides, id)]; \
            type sig = (float)variance[bcast_strides_index(args.varianceStrides, id)]; \
            type g = (float)outGrad[bcast_strides_index(args.outGradStrides, id)]; \
            output[id] = (type)(-g * 0.5 * (x - mu) * pow(sig + args.epsilon, -1.5)); \
        } \
    }

DEFINE_NORMALIZE(half)
DEFINE_NORMALIZE(float)

#define DEFINE_ADAMW(type) \
    kernel void adamw_##type( \
        device const type* param [[buffer(0)]], \
        device const type* grad [[buffer(1)]], \
        device const type* moment1 [[buffer(2)]], \
        device const type* moment2 [[buffer(3)]], \
        device type* newParam [[buffer(4)]], \
        device type* newMoment1 [[buffer(5)]], \
        device type* newMoment2 [[buffer(6)]], \
        constant float& beta1 [[buffer(7)]], \
        constant float& beta2 [[buffer(8)]], \
        constant float& eps [[buffer(9)]], \
        constant float& weightDecay [[buffer(10)]], \
        constant float& lr [[buffer(11)]], \
        constant float& step [[buffer(12)]], \
        constant uint& N [[buffer(13)]], \
        uint id [[thread_position_in_grid]] \
    ) { \
        if (id < N) { \
            type x = (float)param[id]; \
            type g = (float)grad[id]; \
            type mt = (float)moment1[id]; \
            type vt = (float)moment2[id]; \
            mt = beta1 * mt + (1 - beta1) * g; \
            vt = beta2 * vt + (1 - beta2) * g * g; \
            newMoment1[id] = (type)mt; \
            newMoment2[id] = (type)vt; \
            mt = mt / (1 - pow(beta1, step)); \
            vt = vt / (1 - pow(beta2, step)); \
            x = (1 - lr * weightDecay) * x; \
            x = x - lr * mt / (sqrt(vt) + eps); \
            newParam[id] = (type)x; \
        } \
    }

DEFINE_ADAMW(half)
DEFINE_ADAMW(float)

#define DEFINE_CLAMP(type, minMaxType) \
    kernel void clamp_##type( \
        device const type* a [[buffer(0)]], \
        device type* output [[buffer(1)]], \
        device minMaxType& min [[buffer(2)]], \
        device minMaxType& max [[buffer(3)]], \
        device uint& hasMin [[buffer(4)]], \
        device uint& hasMax [[buffer(5)]], \
        constant uint &N [[buffer(6)]], \
        uint id [[thread_position_in_grid]] \
    ) { \
        if (id < N) { \
            float input = a[id]; \
            if (hasMin && input < (type)min) { \
                input = (type)min; \
            } \
            if (hasMax && input > (type)max) { \
                input = (type)max; \
            } \
            output[id] = input; \
        } \
    }

DEFINE_CLAMP(float, float)
DEFINE_CLAMP(long, long)
DEFINE_CLAMP(half, float)

#define DEFINE_WHEN(type) \
    struct when_args_##type { \
        struct bcast_strides maskStrides; \
        struct bcast_strides trueStrides; \
        struct bcast_strides falseStrides; \
        uint N; \
    }; \
    kernel void when_##type( \
        device const char* mask [[buffer(0)]], \
        device const type* trueIn [[buffer(1)]], \
        device const type* falseIn [[buffer(2)]], \
        device type* output [[buffer(3)]], \
        device when_args_##type& args [[buffer(4)]], \
        uint id [[thread_position_in_grid]] \
    ) { \
        if (id < args.N) { \
            if (mask[bcast_strides_index(args.maskStrides, id)]) { \
                output[id] = trueIn[bcast_strides_index(args.trueStrides, id)]; \
            } else { \
                output[id] = falseIn[bcast_strides_index(args.falseStrides, id)]; \
            } \
        } \
    }

DEFINE_WHEN(char)
DEFINE_WHEN(short)
DEFINE_WHEN(int)
DEFINE_WHEN(long)

#define DEFINE_POW(type) \
    kernel void vector_pow_##type( \
        device const type* input [[buffer(0)]], \
        device type* output [[buffer(1)]], \
        constant float &exponent [[buffer(2)]], \
        constant float &outScale [[buffer(3)]], \
        constant uint &N [[buffer(4)]], \
        uint id [[thread_position_in_grid]] \
    ) { \
        if (id < N) { \
            output[id] = (type)(outScale * pow((float)input[id], exponent)); \
        } \
    } \
    kernel void vector_pow_scaled_##type( \
        device const type* input [[buffer(0)]], \
        constant type *outScales [[buffer(1)]], \
        device type* output [[buffer(2)]], \
        constant float &exponent [[buffer(3)]], \
        constant float &outScale [[buffer(4)]], \
        constant uint &N [[buffer(5)]], \
        uint id [[thread_position_in_grid]] \
    ) { \
        if (id < N) { \
            float sc = outScale * (float)outScales[id]; \
            output[id] = (type)(sc * pow((float)input[id], exponent)); \
        } \
    }

DEFINE_POW(half)
DEFINE_POW(float)

float erf(float a) {
  // https://github.com/ml-explore/mlx/blob/0d5e7716ad0adadae215ece6eb70861a6a8b55a3/mlx/backend/common/ops.h#L47
  float r, s, t, u;
  t = abs(a);
  s = a * a;
  if (t > 0.927734375f) {
    // maximum error 0.99527 ulp
    r = fma(
        -1.72853470e-5f, t, 3.83197126e-4f); // -0x1.220000p-16,0x1.91cfb2p-12
    u = fma(
        -3.88396438e-3f, t, 2.42546219e-2f); // -0x1.fd1438p-9, 0x1.8d6342p-6
    r = fma(r, s, u);
    r = fma(r, t, -1.06777877e-1f); // -0x1.b55cb8p-4
    r = fma(r, t, -6.34846687e-1f); // -0x1.450aa0p-1
    r = fma(r, t, -1.28717512e-1f); // -0x1.079d0cp-3
    r = fma(r, t, -t);
    r = 1.0f - exp(r);
    r = copysign(r, a);
  } else {
    // maximum error 0.98929 ulp
    r = -5.96761703e-4f; // -0x1.38e000p-11
    r = fma(r, s, 4.99119423e-3f); //  0x1.471a58p-8
    r = fma(r, s, -2.67681349e-2f); // -0x1.b691b2p-6
    r = fma(r, s, 1.12819925e-1f); //  0x1.ce1c44p-4
    r = fma(r, s, -3.76125336e-1f); // -0x1.812700p-2
    r = fma(r, s, 1.28379166e-1f); //  0x1.06eba8p-3
    r = fma(r, a, a);
  }
  return r;
}

float erf_grad(float x) {
    return abs(x) > 20 ? 0 : 1.1283791670955126 * exp(-x * x);
}

#define ELEMWISE_KERNELS(type) \
    kernel void log_##type( \
        device const type* input [[buffer(0)]], \
        device type* output [[buffer(1)]], \
        constant uint &N [[buffer(2)]], \
        uint id [[thread_position_in_grid]] \
    ) { \
        if (id < N) { \
            output[id] = log(input[id]); \
        } \
    } \
    \
    kernel void recip_##type( \
        device const type* input [[buffer(0)]], \
        device type* output [[buffer(1)]], \
        constant uint &N [[buffer(2)]], \
        uint id [[thread_position_in_grid]] \
    ) { \
        if (id < N) { \
            output[id] = 1 / input[id]; \
        } \
    } \
    \
    kernel void exp_##type( \
        device const type* input [[buffer(0)]], \
        device type* output [[buffer(1)]], \
        constant uint &N [[buffer(2)]], \
        uint id [[thread_position_in_grid]] \
    ) { \
        if (id < N) { \
            output[id] = exp(input[id]); \
        } \
    } \
    \
    kernel void sigmoid_##type( \
        device const type* input [[buffer(0)]], \
        device type* output [[buffer(1)]], \
        constant uint &N [[buffer(2)]], \
        uint id [[thread_position_in_grid]] \
    ) { \
        if (id < N) { \
            output[id] = type((safe_tanh(float(input[id]) / 2) + 1) / 2); \
        } \
    } \
    \
    kernel void sigmoid_grad_##type( \
        device const type* input [[buffer(0)]], \
        device type* output [[buffer(1)]], \
        constant uint &N [[buffer(2)]], \
        uint id [[thread_position_in_grid]] \
    ) { \
        if (id < N) { \
            float s = (safe_tanh((float)input[id] / 2) + 1) / 2; \
            output[id] = type(s * (1 - s)); \
        } \
    } \
    \
    kernel void gelu_approx_##type( \
        device const type* input [[buffer(0)]], \
        device type* output [[buffer(1)]], \
        constant uint &N [[buffer(2)]], \
        uint id [[thread_position_in_grid]] \
    ) { \
        if (id < N) { \
            float x = (float)input[id]; \
            output[id] = type(0.5 * x * (1.0 + safe_tanh(0.797884561 * (x + 0.044715 * pow(x, 3.0))))); \
        } \
    } \
    \
    kernel void erf_##type( \
        device const type* input [[buffer(0)]], \
        device type* output [[buffer(1)]], \
        constant uint &N [[buffer(2)]], \
        uint id [[thread_position_in_grid]] \
    ) { \
        if (id < N) { \
            float x = (float)input[id]; \
            output[id] = type(erf(x)); \
        } \
    } \
    \
    kernel void erf_grad_##type( \
        device const type* input [[buffer(0)]], \
        device type* output [[buffer(1)]], \
        constant uint &N [[buffer(2)]], \
        uint id [[thread_position_in_grid]] \
    ) { \
        if (id < N) { \
            float x = (float)input[id]; \
            output[id] = (type)erf_grad(x); \
        } \
    } \
    \
    kernel void gelu_approx_grad_##type( \
        device const type* input [[buffer(0)]], \
        device type* output [[buffer(1)]], \
        constant uint &N [[buffer(2)]], \
        uint id [[thread_position_in_grid]] \
    ) { \
        if (id < N) { \
            float x = (float)input[id]; \
            float tanhTerm = safe_tanh(0.035677408145115 * pow(x, 3.0) + 0.797884561 * x); \
            output[id] = type(0.5 * x * (1.0 - pow(tanhTerm, 2.0)) * (0.107032224435345 * pow(x, 2.0) + 0.797884561) + 0.5 * tanhTerm + 0.5); \
        } \
    } \
    \
    kernel void gelu_exact_##type( \
        device const type* input [[buffer(0)]], \
        device type* output [[buffer(1)]], \
        constant uint &N [[buffer(2)]], \
        uint id [[thread_position_in_grid]] \
    ) { \
        if (id < N) { \
            float x = (float)input[id]; \
            output[id] = type(x * 0.5 * (1 + erf(x * 0.7071067811865475))); \
        } \
    } \
    \
    kernel void gelu_exact_grad_##type( \
        device const type* input [[buffer(0)]], \
        device type* output [[buffer(1)]], \
        constant uint &N [[buffer(2)]], \
        uint id [[thread_position_in_grid]] \
    ) { \
        if (id < N) { \
            float x = (float)input[id]; \
            float c = 0.7071067811865475; \
            output[id] = type(0.5 * (1 + erf(x * c)) + 0.5 * x * c * erf_grad(x * c)); \
        } \
    } \
    \
    kernel void sin_##type( \
        device const type* input [[buffer(0)]], \
        device type* output [[buffer(1)]], \
        constant uint &N [[buffer(2)]], \
        uint id [[thread_position_in_grid]] \
    ) { \
        if (id < N) { \
            output[id] = sin(input[id]); \
        } \
    } \
    \
    kernel void cos_##type( \
        device const type* input [[buffer(0)]], \
        device type* output [[buffer(1)]], \
        constant uint &N [[buffer(2)]], \
        uint id [[thread_position_in_grid]] \
    ) { \
        if (id < N) { \
            output[id] = cos(input[id]); \
        } \
    } \
    \
    kernel void minus_sin_##type( \
        device const type* input [[buffer(0)]], \
        device type* output [[buffer(1)]], \
        constant uint &N [[buffer(2)]], \
        uint id [[thread_position_in_grid]] \
    ) { \
        if (id < N) { \
            output[id] = -sin(input[id]); \
        } \
    } \
    \
    kernel void tan_##type( \
        device const type* input [[buffer(0)]], \
        device type* output [[buffer(1)]], \
        constant uint &N [[buffer(2)]], \
        uint id [[thread_position_in_grid]] \
    ) { \
        if (id < N) { \
            output[id] = tan(input[id]); \
        } \
    } \
    \
    kernel void tan_grad_##type( \
        device const type* input [[buffer(0)]], \
        device type* output [[buffer(1)]], \
        constant uint &N [[buffer(2)]], \
        uint id [[thread_position_in_grid]] \
    ) { \
        if (id < N) { \
            output[id] = 1 / pow(cos(input[id]), 2); \
        } \
    } \
    \
    kernel void atan_##type( \
        device const type* input [[buffer(0)]], \
        device type* output [[buffer(1)]], \
        constant uint &N [[buffer(2)]], \
        uint id [[thread_position_in_grid]] \
    ) { \
        if (id < N) { \
            output[id] = atan(input[id]); \
        } \
    } \
    \
    kernel void atan_grad_##type( \
        device const type* input [[buffer(0)]], \
        device type* output [[buffer(1)]], \
        constant uint &N [[buffer(2)]], \
        uint id [[thread_position_in_grid]] \
    ) { \
        if (id < N) { \
            output[id] = 1 / (1 + pow(input[id], 2)); \
        } \
    } \
    \
    kernel void acos_##type( \
        device const type* input [[buffer(0)]], \
        device type* output [[buffer(1)]], \
        constant uint &N [[buffer(2)]], \
        uint id [[thread_position_in_grid]] \
    ) { \
        if (id < N) { \
            output[id] = acos(input[id]); \
        } \
    } \
    \
    kernel void acos_grad_##type( \
        device const type* input [[buffer(0)]], \
        device type* output [[buffer(1)]], \
        constant uint &N [[buffer(2)]], \
        uint id [[thread_position_in_grid]] \
    ) { \
        if (id < N) { \
            output[id] = -1 / sqrt(1 - pow(input[id], 2)); \
        } \
    } \
    \
    kernel void asin_##type( \
        device const type* input [[buffer(0)]], \
        device type* output [[buffer(1)]], \
        constant uint &N [[buffer(2)]], \
        uint id [[thread_position_in_grid]] \
    ) { \
        if (id < N) { \
            output[id] = asin(input[id]); \
        } \
    } \
    \
    kernel void asin_grad_##type( \
        device const type* input [[buffer(0)]], \
        device type* output [[buffer(1)]], \
        constant uint &N [[buffer(2)]], \
        uint id [[thread_position_in_grid]] \
    ) { \
        if (id < N) { \
            output[id] = 1 / sqrt(1 - pow(input[id], 2)); \
        } \
    } \
    \
    kernel void relu_##type( \
        device const type* input [[buffer(0)]], \
        device type* output [[buffer(1)]], \
        constant uint &N [[buffer(2)]], \
        uint id [[thread_position_in_grid]] \
    ) { \
        if (id < N) { \
            float x = input[id]; \
            output[id] = x > 0.0f ? x : 0.0f; \
        } \
    } \
    \
    kernel void relu_grad_##type( \
        device const type* input [[buffer(0)]], \
        device type* output [[buffer(1)]], \
        constant uint &N [[buffer(2)]], \
        uint id [[thread_position_in_grid]] \
    ) { \
        if (id < N) { \
            float x = input[id]; \
            output[id] = x > 0.0f ? 1.0f : 0.0f; \
        } \
    } \
    \
    kernel void abs_##type( \
        device const type* input [[buffer(0)]], \
        device type* output [[buffer(1)]], \
        constant uint &N [[buffer(2)]], \
        uint id [[thread_position_in_grid]] \
    ) { \
        if (id < N) { \
            output[id] = abs(input[id]); \
        } \
    } \
    \
    kernel void abs_grad_##type( \
        device const type* input [[buffer(0)]], \
        device type* output [[buffer(1)]], \
        constant uint &N [[buffer(2)]], \
        uint id [[thread_position_in_grid]] \
    ) { \
        if (id < N) { \
            output[id] = input[id] < 0 ? -1 : 1; \
        } \
    } \
    \
    kernel void floor_##type( \
        device const type* input [[buffer(0)]], \
        device type* output [[buffer(1)]], \
        constant uint &N [[buffer(2)]], \
        uint id [[thread_position_in_grid]] \
    ) { \
        if (id < N) { \
            output[id] = floor(input[id]); \
        } \
    } \
    \
    kernel void ceil_##type( \
        device const type* input [[buffer(0)]], \
        device type* output [[buffer(1)]], \
        constant uint &N [[buffer(2)]], \
        uint id [[thread_position_in_grid]] \
    ) { \
        if (id < N) { \
            output[id] = ceil(input[id]); \
        } \
    } \
    \
    kernel void round_##type( \
        device const type* input [[buffer(0)]], \
        device type* output [[buffer(1)]], \
        constant uint &N [[buffer(2)]], \
        uint id [[thread_position_in_grid]] \
    ) { \
        if (id < N) { \
            output[id] = round(input[id]); \
        } \
    }

ELEMWISE_KERNELS(float)
ELEMWISE_KERNELS(half)

#define DEFINE_REPEAT(type) \
    struct repeat_args_##type { \
        uint inner; \
        uint outer; \
        uint reps; \
    }; \
    kernel void repeat_##type( \
        device const type* input [[buffer(0)]], \
        device type* output [[buffer(1)]], \
        constant struct repeat_args_##type &args [[buffer(2)]], \
        uint id [[thread_position_in_grid]] \
    ) { \
        if (id < args.inner * args.outer * args.reps) { \
            uint sourceIdx = (id % args.inner) + (id / (args.inner * args.reps)) * args.inner; \
            output[id] = input[sourceIdx]; \
        } \
    }

DEFINE_REPEAT(char)
DEFINE_REPEAT(short)
DEFINE_REPEAT(int)
DEFINE_REPEAT(long)

#define DEFINE_STRIDED_COPY(type) \
    struct strided_copy_args_##type { \
        uint inner; \
        uint fullInner; \
        uint outer; \
        uint offset; \
    }; \
    kernel void strided_copy_##type( \
        device const type* input [[buffer(0)]], \
        device type* output [[buffer(1)]], \
        constant struct strided_copy_args_##type &args [[buffer(2)]], \
        uint id [[thread_position_in_grid]] \
    ) { \
        if (id < args.inner * args.outer) { \
            uint sourceCol = id % args.inner; \
            uint sourceRow = id / args.inner; \
            output[(sourceCol + args.offset) + sourceRow*args.fullInner] = input[id]; \
        } \
    }

DEFINE_STRIDED_COPY(char)
DEFINE_STRIDED_COPY(short)
DEFINE_STRIDED_COPY(int)
DEFINE_STRIDED_COPY(long)

constant uint PHILOX_ROUND_A = 0xD2511F53;
constant uint PHILOX_ROUND_B = 0xCD9E8D57;
constant uint PHILOX_KEY_A = 0x9E3779B9;
constant uint PHILOX_KEY_B = 0xBB67AE85;
constant constexpr float M_PI = 3.14159265358979323846264338327950288;

inline uint umulhi(uint x, uint y) {
    ulong prod = ((ulong)x) * ((ulong)y);
    return (uint)(prod >> 32);
}

inline void philox(ulong seed, ulong offset, thread uint* c) {
    c[0] = (uint)(offset & 0xffffffff);
    c[1] = (uint)(offset >> 32);
    c[2] = 0;
    c[3] = 0;
    uint k0 = (uint)(seed & 0xffffffff);
    uint k1 = (uint)(seed >> 32);
  
    for (int i = 0; i < 10; i++) {
        uint prev_c0 = c[0];
        uint prev_c2 = c[2];
        c[0] = umulhi(PHILOX_ROUND_B, c[2]) ^ c[1] ^ k0;
        c[2] = umulhi(PHILOX_ROUND_A, prev_c0) ^ c[3] ^ k1;
        c[1] = PHILOX_ROUND_B * prev_c2;
        c[3] = PHILOX_ROUND_A * prev_c0;
        k0 = (k0 + PHILOX_KEY_A);
        k1 = (k1 + PHILOX_KEY_B);
    }
}

constant uint64_t MAX_ULONG = 0xFFFFFFFFFFFFFFFF;

ulong next_pow_of_two(ulong x) {
    if (x == 0) {
        return 1;
    }

    if (x >= (MAX_ULONG >> 1)) {
        return MAX_ULONG;
    }

    x--;
    x |= x >> 1;
    x |= x >> 2;
    x |= x >> 4;
    x |= x >> 8;
    x |= x >> 16;
    x |= x >> 32;
    return x + 1;
}

kernel void rand_long(
    const device ulong* state [[buffer(0)]],
    device ulong* outState [[buffer(1)]],
    device ulong* output [[buffer(2)]],
    constant ulong &minVal [[buffer(3)]],
    constant ulong &count [[buffer(4)]],
    constant uint &size [[buffer(5)]],
    uint id [[thread_position_in_grid]]
) {
    ulong sample = 0;
    uint found = 0;
    uint c[4];

    ulong bound = next_pow_of_two(count);

    if (id == 0) {
        outState[0] = state[0];
        outState[1] = state[1] + ((ulong)size) * 32;
    }

    for (int i = 0; i < 32; i++) {
        philox(state[0], state[1] + i + id*32, c);
        ulong v1 = (((ulong)c[0]) | (((ulong)c[1]) << 32)) % bound;
        ulong v2 = (((ulong)c[2]) | (((ulong)c[3]) << 32)) % bound;
        if (v1 < count && !found) {
            found = 1;
            sample = v1;
        }
        if (v2 < count && !found) {
            found = 1;
            sample = v2;
        }
    }
    if (id < size) {
        output[id] = sample + minVal;
    }
}

#define DEFINE_RAND(type) \
    kernel void rand_##type( \
        const device ulong* state [[buffer(0)]], \
        device ulong* outState [[buffer(1)]], \
        device type* output [[buffer(2)]], \
        constant uint &size [[buffer(3)]], \
        uint id [[thread_position_in_grid]] \
    ) { \
        uint c[4]; \
        philox(state[0], state[1] + id, c); \
        if (id == 0) { \
            outState[0] = state[0]; \
            outState[1] = state[1] + ((ulong)size + 3) / 4; \
        } \
        for (int i = 0; i < 4; i++) { \
            uint outIdx = id * 4 + i; \
            if (outIdx < size) { \
                output[outIdx] = (type)(float(c[i]) / float(0xffffffff)); \
            } \
        } \
    } \
    kernel void randn_##type( \
        const device ulong* state [[buffer(0)]], \
        device ulong* outState [[buffer(1)]], \
        device type* output [[buffer(2)]], \
        constant uint &size [[buffer(3)]], \
        uint id [[thread_position_in_grid]] \
    ) { \
        if (id == 0) { \
            outState[0] = state[0]; \
            outState[1] = state[1] + ((ulong)size + 1) / 2; \
        } \
        uint c[4]; \
        philox(state[0], state[1] + id, c); \
        float u1 = float(c[0]) / float(0xffffffff); \
        if (u1 < 1e-5) { \
            u1 = 1e-5; \
        } \
        float u2 = float(c[1]) / float(0xffffffff); \
        float r = sqrt(-2 * log(u1)); \
        float phi = 2 * M_PI * u2; \
        float z[2]; \
        z[0] = r * cos(phi); \
        z[1] = r * sin(phi); \
        for (int i = 0; i < 2; i++) { \
            uint outIdx = id * 2 + i; \
            if (outIdx < size) { \
                output[outIdx] = (type)z[i]; \
            } \
        } \
    }

DEFINE_RAND(float)
DEFINE_RAND(half)

#define DEFINE_GATHER(gather, type) \
    kernel void gather##_##type( \
        device const type* input [[buffer(0)]], \
        device const ulong* indices [[buffer(1)]], \
        device type* output [[buffer(2)]], \
        constant uint &outerCount [[buffer(3)]], \
        constant uint &indexCount [[buffer(4)]], \
        constant uint &middleCount [[buffer(5)]], \
        constant uint &innerCount [[buffer(6)]], \
        constant struct bcast_strides &indexStrides [[buffer(7)]], \
        uint id [[thread_position_in_grid]] \
    ) { \
        uint innerIdx = id % innerCount; \
        uint outerIdx = (id / innerCount) / indexCount; \
        if (outerIdx < outerCount) { \
            ulong index = indices[bcast_strides_index(indexStrides, id)]; \
            ulong srcIndex = innerIdx + index*innerCount + outerIdx*innerCount*middleCount; \
            output[id] = input[srcIndex]; \
        } \
    }

DEFINE_GATHER(gather, char)
DEFINE_GATHER(gather, short)
DEFINE_GATHER(gather, int)
DEFINE_GATHER(gather, long)

#define DEFINE_SCATTER(scatter, type) \
    kernel void scatter##_##type( \
        device const type* input [[buffer(0)]], \
        device const ulong* indices [[buffer(1)]], \
        device type* output [[buffer(2)]], \
        constant uint &outerCount [[buffer(3)]], \
        constant uint &indexCount [[buffer(4)]], \
        constant uint &middleCount [[buffer(5)]], \
        constant uint &innerCount [[buffer(6)]], \
        constant struct bcast_strides &indexStrides [[buffer(7)]], \
        uint id [[thread_position_in_grid]] \
    ) { \
        uint innerIdx = id % innerCount; \
        uint outerIdx = (id / innerCount) / indexCount; \
        if (outerIdx < outerCount) { \
            ulong index = indices[bcast_strides_index(indexStrides, id)]; \
            ulong dstIndex = innerIdx + index*innerCount + outerIdx*innerCount*middleCount; \
            output[dstIndex] = input[id]; \
        } \
    }

DEFINE_SCATTER(scatter, char)
DEFINE_SCATTER(scatter, short)
DEFINE_SCATTER(scatter, int)
DEFINE_SCATTER(scatter, long)

kernel void axis_permutation(
    device const uint* newStrides [[buffer(0)]],
    device const uint* newShape [[buffer(1)]],
    device const uint* permutedStrides [[buffer(2)]],
    device ulong* output [[buffer(3)]],
    constant uint &numAxes [[buffer(4)]],
    constant uint &outputCount [[buffer(5)]],
    uint id [[thread_position_in_grid]]
) {
    uint flatIndex = 0;
    for (uint i = 0; i < numAxes; i++) {
        uint oldIdx = (id / newStrides[i]) % newShape[i];
        flatIndex += oldIdx * permutedStrides[i];
    }
    if (id < outputCount) {
        output[id] = (ulong)flatIndex;
    }
}

#define DEFINE_CAST(inType, outType) \
    kernel void cast_##inType##_##outType( \
        device const inType* input [[buffer(0)]], \
        device outType* output [[buffer(1)]], \
        constant uint &count [[buffer(2)]], \
        uint id [[thread_position_in_grid]] \
    ) { \
        if (id < count) { \
            output[id] = (outType)input[id]; \
        } \
    }

DEFINE_CAST(float, half)
DEFINE_CAST(float, long)
DEFINE_CAST(half, float)
DEFINE_CAST(half, long)
DEFINE_CAST(long, float)
DEFINE_CAST(long, half)

#define DEFINE_LOG_SOFTMAX(type) \
    kernel void log_softmax_##type( \
        device const type* input [[buffer(0)]], \
        device type* output [[buffer(1)]], \
        constant uint &numCols [[buffer(2)]], \
        uint blockIdx [[threadgroup_position_in_grid]], \
        uint idxInBlock [[thread_position_in_threadgroup]] \
    ) { \
        const uint blockSize = 256; \
        threadgroup float reduceBuffer[256]; \
    \
        uint rowOffset = blockIdx * numCols; \
        float localMax = -INFINITY; \
        for (uint i = 0; i * blockSize < numCols; i++) { \
            uint col = i * blockSize + idxInBlock; \
            if (col < numCols) { \
                float x = (float)input[rowOffset + col]; \
                localMax = localMax < x ? x : localMax; \
            } \
        } \
    \
        for (uint i = 0; i < 8; i++) { \
            reduceBuffer[idxInBlock] = localMax; \
            threadgroup_barrier(mem_flags::mem_threadgroup); \
    \
            uint otherIdx = idxInBlock ^ (1 << i); \
            float otherValue = reduceBuffer[otherIdx]; \
            localMax = localMax < otherValue ? otherValue : localMax; \
            threadgroup_barrier(mem_flags::mem_threadgroup); \
        } \
    \
        float localSum = 0.0; \
        for (uint i = 0; i * blockSize < numCols; i++) { \
            uint col = i * blockSize + idxInBlock; \
            if (col < numCols) { \
                localSum += exp((float)input[rowOffset + col] - localMax); \
            } \
        } \
    \
        for (uint i = 0; i < 8; i++) { \
            reduceBuffer[idxInBlock] = localSum; \
            threadgroup_barrier(mem_flags::mem_threadgroup); \
    \
            uint otherIdx = idxInBlock ^ (1 << i); \
            float otherValue = reduceBuffer[otherIdx]; \
            localSum = localSum + otherValue; \
            threadgroup_barrier(mem_flags::mem_threadgroup); \
        } \
    \
        float finalLogSum = log(localSum) + localMax; \
        for (uint i = 0; i * blockSize < numCols; i++) { \
            uint col = i * blockSize + idxInBlock; \
            if (col < numCols) { \
                float x = (float)input[rowOffset + col]; \
                output[rowOffset + col] = (type)(x - finalLogSum); \
            } \
        } \
    } \
    kernel void log_softmax_grad_##type( \
        device const type* input [[buffer(0)]], \
        device const type* grad [[buffer(1)]], \
        device type* output [[buffer(2)]], \
        constant uint &numCols [[buffer(3)]], \
        uint blockIdx [[threadgroup_position_in_grid]], \
        uint idxInBlock [[thread_position_in_threadgroup]] \
    ) { \
        const uint blockSize = 256; \
        threadgroup float reduceBuffer[256]; \
    \
        uint rowOffset = blockIdx * numCols; \
        float localMax = -INFINITY; \
        for (uint i = 0; i * blockSize < numCols; i++) { \
            uint col = i * blockSize + idxInBlock; \
            if (col < numCols) { \
                float x = (float)input[rowOffset + col]; \
                localMax = localMax < x ? x : localMax; \
            } \
        } \
    \
        for (uint i = 0; i < 8; i++) { \
            reduceBuffer[idxInBlock] = localMax; \
            threadgroup_barrier(mem_flags::mem_threadgroup); \
    \
            uint otherIdx = idxInBlock ^ (1 << i); \
            float otherValue = reduceBuffer[otherIdx]; \
            localMax = localMax < otherValue ? otherValue : localMax; \
            threadgroup_barrier(mem_flags::mem_threadgroup); \
        } \
    \
        float localSum = 0.0; \
        for (uint i = 0; i * blockSize < numCols; i++) { \
            uint col = i * blockSize + idxInBlock; \
            if (col < numCols) { \
                localSum += exp((float)input[rowOffset + col] - localMax); \
            } \
        } \
    \
        for (uint i = 0; i < 8; i++) { \
            reduceBuffer[idxInBlock] = localSum; \
            threadgroup_barrier(mem_flags::mem_threadgroup); \
    \
            uint otherIdx = idxInBlock ^ (1 << i); \
            float otherValue = reduceBuffer[otherIdx]; \
            localSum = localSum + otherValue; \
            threadgroup_barrier(mem_flags::mem_threadgroup); \
        } \
    \
        float finalLogSum = log(localSum) + localMax; \
        float localGradSum = 0.0; \
        for (uint i = 0; i * blockSize < numCols; i++) { \
            uint col = i * blockSize + idxInBlock; \
            if (col < numCols) { \
                localGradSum += (float)grad[rowOffset + col]; \
            } \
        } \
    \
        for (uint i = 0; i < 8; i++) { \
            reduceBuffer[idxInBlock] = localGradSum; \
            threadgroup_barrier(mem_flags::mem_threadgroup); \
    \
            uint otherIdx = idxInBlock ^ (1 << i); \
            float otherValue = reduceBuffer[otherIdx]; \
            localGradSum = localGradSum + otherValue; \
            threadgroup_barrier(mem_flags::mem_threadgroup); \
        } \
        for (uint i = 0; i * blockSize < numCols; i++) { \
            uint col = i * blockSize + idxInBlock; \
            if (col < numCols) { \
                float x = (float)input[rowOffset + col]; \
                float prob = exp(x - finalLogSum); \
                float g = (float)grad[rowOffset + col]; \
                output[rowOffset + col] = (type)(g - localGradSum * prob); \
            } \
        } \
    }

DEFINE_LOG_SOFTMAX(half)
DEFINE_LOG_SOFTMAX(float)

inline float reduceSumLocal(uint blockSize, uint iters, threadgroup float* reduceBuffer, uint idxInBlock, float value) {
    for (uint i = 0; i < iters; i++) {
        reduceBuffer[idxInBlock] = value;
        threadgroup_barrier(mem_flags::mem_threadgroup);
        uint otherIdx = idxInBlock ^ (1 << i);
        float otherValue = reduceBuffer[otherIdx];
        value += otherValue;
        threadgroup_barrier(mem_flags::mem_threadgroup);
    }
    return value;
}

#define DEFINE_NORMALIZE_INNER(type) \
    kernel void normalize_inner_##type( \
        device const type* input [[buffer(0)]], \
        device type* output [[buffer(1)]], \
        constant float &epsilon [[buffer(2)]], \
        constant uint &numCols [[buffer(3)]], \
        uint blockIdx [[threadgroup_position_in_grid]], \
        uint idxInBlock [[thread_position_in_threadgroup]] \
    ) { \
        const uint blockSize = 256; \
        threadgroup float reduceBuffer[256]; \
        \
        uint rowOffset = blockIdx * numCols; \
        float localSum = 0.0; \
        float localSumSq = 0.0; \
        for (uint i = 0; i * blockSize < numCols; i++) { \
            uint col = i * blockSize + idxInBlock; \
            if (col < numCols) { \
                float x = (float)input[rowOffset + col]; \
                localSum += x; \
                localSumSq += x * x; \
            } \
        } \
        localSum = reduceSumLocal(blockSize, 8, reduceBuffer, idxInBlock, localSum); \
        localSumSq = reduceSumLocal(blockSize, 8, reduceBuffer, idxInBlock, localSumSq); \
        float mean = localSum / (float)numCols; \
        float variance = (localSumSq / (float)numCols) - mean * mean; \
        if (variance < 0) { \
            variance = 0; \
        } \
        float factor = 1 / sqrt(variance + epsilon); \
        for (uint i = 0; i * blockSize < numCols; i++) { \
            uint col = i * blockSize + idxInBlock; \
            if (col < numCols) { \
                float x = (float)input[rowOffset + col]; \
                float y = (x - mean) * factor; \
                output[rowOffset + col] = y; \
            } \
        } \
    } \
    kernel void normalize_inner_grad_##type( \
        device const type* input [[buffer(0)]], \
        device const type* outGrad [[buffer(1)]], \
        device type* output [[buffer(2)]], \
        constant float &epsilon [[buffer(3)]], \
        constant uint &numCols [[buffer(4)]], \
        uint blockIdx [[threadgroup_position_in_grid]], \
        uint idxInBlock [[thread_position_in_threadgroup]] \
    ) { \
        const uint blockSize = 256; \
        threadgroup float reduceBuffer[256]; \
        \
        uint rowOffset = blockIdx * numCols; \
        float localSum = 0.0; \
        float localSumSq = 0.0; \
        float localGradSum = 0.0; \
        float localCov = 0.0; \
        for (uint i = 0; i * blockSize < numCols; i++) { \
            uint col = i * blockSize + idxInBlock; \
            if (col < numCols) { \
                float x = (float)input[rowOffset + col]; \
                float y = (float)outGrad[rowOffset + col]; \
                localSum += x; \
                localSumSq += x * x; \
                localGradSum += y; \
                localCov += x * y; \
            } \
        } \
        localSum = reduceSumLocal(blockSize, 8, reduceBuffer, idxInBlock, localSum); \
        localSumSq = reduceSumLocal(blockSize, 8, reduceBuffer, idxInBlock, localSumSq); \
        localGradSum = reduceSumLocal(blockSize, 8, reduceBuffer, idxInBlock, localGradSum); \
        localCov = reduceSumLocal(blockSize, 8, reduceBuffer, idxInBlock, localCov); \
        float mean = localSum / (float)numCols; \
        float variance = (localSumSq / (float)numCols) - mean * mean; \
        float gradMean = localGradSum / (float)numCols; \
        localCov -= localGradSum * mean; \
        localCov /= (float)numCols; \
        if (variance < 0) { \
            variance = 0; \
        } \
        float factor = 1 / sqrt(variance + epsilon); \
        float factor3 = factor * factor * factor; \
        for (uint i = 0; i * blockSize < numCols; i++) { \
            uint col = i * blockSize + idxInBlock; \
            if (col < numCols) { \
                float x = (float)input[rowOffset + col]; \
                float g = (float)outGrad[rowOffset + col]; \
                float y = (g - gradMean) * factor - (x - mean) * localCov * factor3; \
                output[rowOffset + col] = y; \
            } \
        } \
    }

DEFINE_NORMALIZE_INNER(float)
DEFINE_NORMALIZE_INNER(half)